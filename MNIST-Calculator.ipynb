{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Calculator\n",
    "\n",
    "This notebook can be used to reproduce all the experiments for the paper _Performing Arithmetic Using a Neural Network Trained on Digit Permutation Pairs_. See <https://link.springer.com/chapter/10.1007/978-3-030-59491-6_24> for details.\n",
    "\n",
    "First we import the packages we require:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras imports for the convolutional neural network\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "\n",
    "# Additional packages\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import utils \n",
    "from sklearn.model_selection import KFold\n",
    "import IPython.display as dp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plt.rcParams['figure.figsize'] = (18, 6)\n",
    "from scipy.stats import itemfreq\n",
    "from math import floor, ceil\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Generate Training and Test Data\n",
    "\n",
    "Now that the appropriate libraries have been loaded, we will use scikit-learn's built-in MNIST dataset for generating the data in these experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_keras, y_train_keras), (X_test_keras, y_test_keras) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset has been loaded, we can take a look at an image from the dataset like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(X_train_keras[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST images are $28 \\times 28$ pixels in size, and the combined images that will be used for training and testing will be two MNIST images placed side by side and merged as a single image. The dimensions of a single MNIST image can be seen as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_keras[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the images used in this experiment will be $28 \\times 56$ in size. \n",
    "\n",
    "To demonstrate how the combined images look, we can make a random combination and display it inline. First we create an empty $28 \\times 56$ image matrix and then populate it with two images from the MNIST training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty 28 * 56 element matrix\n",
    "test_image = np.zeros((28,56), dtype=\"uint8\")\n",
    "\n",
    "# Populate the empty matrix with data from two image matrices from the training set\n",
    "test_image[:,:28] = X_train_keras[0]\n",
    "test_image[:,28:] = X_train_keras[1]\n",
    "Image.fromarray(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, to generate the data we merely combine two MNIST images into one image.\n",
    "\n",
    "Now we generate our trainging pairs and test pairs **indices**. \n",
    "\n",
    "For the digits 0-9, there are 100 possible combinations of digits. More formally, we can say that the left side image can consist of the digits 0-9: $S_l = \\{0,1,2,3,4,5,6,7,8,9\\}$ and the right side consist of images from the set $S_r = \\{0,1,2,3,4,5,6,7,8,9\\}$. The total possible number of combinations is the Cartesian product of the two sets $D = S_l \\times S_r$. \n",
    "\n",
    "Here we first generate all possible pairs of combinations and then randomly select 90% of these for the training set while the remaining 10% are used for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the 100 unique pairs\n",
    "unique_pairs = [str(x)+str(y) for x in range(10) for y in range(10)]\n",
    "\n",
    "# Create 10 test set pairs\n",
    "test_set_pairs = []\n",
    "\n",
    "while(len(test_set_pairs) < 10):\n",
    "    pair_to_add = random.choice(unique_pairs)\n",
    "    if pair_to_add not in test_set_pairs:\n",
    "        test_set_pairs.append(pair_to_add)\n",
    "\n",
    "#Use the remaining 90 as training set pairs\n",
    "train_set_pairs = list(set(unique_pairs) - set(test_set_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the test set and training set pairs to ensure there is no overlap between the two data sets. These will be used later to generate the training and test data (they will be used as indices).\n",
    "\n",
    "The test looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_set_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the training set looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also perform some sanity tests on our data created so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are 90 training set pairs and 10 test set pairs\n",
    "assert(len(test_set_pairs) == 10)\n",
    "assert(len(train_set_pairs) == 90)\n",
    "\n",
    "# Ensure no test set pairs appear in the training set pairs:\n",
    "for test_set in test_set_pairs:\n",
    "    assert(test_set not in train_set_pairs)\n",
    "    print(\"%s not in training set.\" % test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our permutation pairs, we can begin generating the data.\n",
    "We use the `train_set_pairs` and `test_set_pairs` arrays as indices to generate data, and in the example below we create 1000 samples for each test and train set permutation pair. So, there are exactly 1000 samples for every premutation, resulting in 100,000 images in total split 90/10 across the train set and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Number of samples per permutation (e.g. there are 90 permutations in the train set so 1000 * 90)\n",
    "samples_per_permutation = 10  # Set to 10 for brevity. Results in the paper were for 1,000 samples.\n",
    "\n",
    "for train_set_pair in train_set_pairs:\n",
    "    for _ in range(samples_per_permutation):\n",
    "        rand_i = np.random.choice(np.where(y_train_keras == int(train_set_pair[0]))[0])\n",
    "        rand_j = np.random.choice(np.where(y_train_keras == int(train_set_pair[1]))[0])\n",
    "        \n",
    "        temp_image = np.zeros((28,56), dtype=\"uint8\")\n",
    "        temp_image[:,:28] = X_train_keras[rand_i]\n",
    "        temp_image[:,28:] = X_train_keras[rand_j]\n",
    "\n",
    "        X_train.append(temp_image)\n",
    "        y_train.append(y_train_keras[rand_i] + y_train_keras[rand_j])\n",
    "        \n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for test_set_pair in test_set_pairs:\n",
    "    for _ in range(samples_per_permutation):\n",
    "        rand_i = np.random.choice(np.where(y_test_keras == int(test_set_pair[0]))[0])\n",
    "        rand_j = np.random.choice(np.where(y_test_keras == int(test_set_pair[1]))[0])\n",
    "        \n",
    "        temp_image = np.zeros((28,56), dtype=\"uint8\")\n",
    "        temp_image[:,:28] = X_test_keras[rand_i]\n",
    "        temp_image[:,28:] = X_test_keras[rand_j]\n",
    "            \n",
    "        X_test.append(temp_image)\n",
    "        y_test.append(y_test_keras[rand_i] + y_test_keras[rand_j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that for the purposes of brevity, we have generated only 10 samples per permutation pair (see the line `samples_per_permutation = 10` above). For the results reported in the paper this was set to 1,000 samples per permutation pair. To replicate more closely the results in the paper, please adjust accordindingly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we also generate our labels, which are the sum of the two images contained in each generated image.\n",
    "\n",
    "Before we preview our data, however, we will ensure we have generated the number of images is correct and that the label data matches the image data sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set size: %s, test set size: %s\" % (len(X_train), len(X_test)))\n",
    "\n",
    "# The training set should be 90,000 images in size (90 permutations * 1000)\n",
    "# and the label data, y_train, must also be equal in length.\n",
    "assert(len(X_train) == samples_per_permutation * 90)\n",
    "assert(len(X_train) == len(y_train))\n",
    "\n",
    "# The test set should be 10,000 images in size (10 permutations * 1000)\n",
    "# and the label data, y_test, must also be equal in length\n",
    "assert(len(X_test) == samples_per_permutation * 10)\n",
    "assert(len(X_test) == len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also preview a generated image and its label so that we get an understanding of how the dataset looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = random.randint(0, len(X_test))\n",
    "print(\"Image label: \" + str(y_test[ind]))\n",
    "Image.fromarray(X_test[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each image sample consists of two MNIST images side by side, with each digit taken at random from the entire set of possible 3s in the MNIST data set. The image's label is the addition of the the two numbers.\n",
    "\n",
    "We can examine the frequency of the labels for the training and test set data also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,c = np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(s)):\n",
    "    print(\"%s: %s\" % (s[i], c[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,c = np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(s)):\n",
    "    print(\"%s: %s\" % (s[i], c[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more images that sum to 7, 8, 9, 10, 11 and so on for obvious reasons.\n",
    "\n",
    "What is important to note is that the network will be trained on 90 of the possible 100 permutations of the digits 0-9, and hence when tested will never have seen those permutations before. As we saw above, the `test_set_pairs` array contains 10 permuations that will never be seen during training.\n",
    "\n",
    "Finally, we will prepare the data for input into the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we are using NumPy arrays\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "# Reshape the data sets to a format suitable for Keras\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "\n",
    "# Reformat the images to use floating point values rather than integers between 0-255\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Shuffling the data is always good practice\n",
    "X_train, y_train = utils.shuffle(X_train, y_train)\n",
    "X_test, y_test = utils.shuffle(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network\n",
    "\n",
    "A very similar network to that used in the seminal LeNet5 experiment is used, except that we will be training this network as a regression problem rather than a classification problem. See the paper linked above for full details.\n",
    "\n",
    "**Note that for the purposes of brevity, we are setting the number of epochs to 20 (see the line `epochs = 20` in the code block below). For the results reported in the paper, the number of epochs was 100. To replicate more closely the results in the paper, please adjust accordindingly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Set up a few constants related to the size of the \n",
    "# network, number of output classes and so on.\n",
    "batch_size = 128\n",
    "# This is a regression problem and we will \n",
    "# use a single neuron as output: this network is not \n",
    "# being trained as a classification problem.\n",
    "num_classes = 1               \n",
    "epochs = 20  # Set to 20 for brevity. See paper for details of number of epochs.\n",
    "img_rows, img_cols = np.shape(X_train)[1], np.shape(X_train)[2]\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "\n",
    "######################################################\n",
    "# Set up the network itself\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))  \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))  \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# Do not use softmax here, just specify one neuron\n",
    "model.add(Dense(num_classes, activation='linear')) \n",
    "\n",
    "\n",
    "######################################################\n",
    "# Choose an optimiser and configure it.\n",
    "# Here we have initialised a number of optimisers\n",
    "# and you can experiment with different ones if required.\n",
    "rms = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "sgd = optimizers.SGD(lr=0.0001, decay=1e-5, momentum=0.9, nesterov=True)\n",
    "ada = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "ndm = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "\n",
    "\n",
    "######################################################\n",
    "# Compile the network.\n",
    "# Note: As this is a regression problem, only mean squared error \n",
    "# or mean absolute error can be used for the loss.\n",
    "model.compile(loss=losses.mean_squared_error, optimizer=ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train the network. This will take some time, especially if you are not using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that training is complete, we can print the network's score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"\\n%.5f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the MSE loss on the training set and test set across the epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlim(0,len(history.history['loss'])-1)\n",
    "plt.plot(history.history['loss'], linestyle='--', linewidth=3)\n",
    "plt.plot(history.history['val_loss'], linewidth=3)\n",
    "plt.title('Loss on Test/Training Set')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Set', 'Test Set (Loss @ Final Epoch: '+ str(\"%.2f\"%score) +')'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can select random images from the test set, and see how well the network predicts their summation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    ind = random.randint(0, len(X_test)-1)\n",
    "    image_predicted = X_test[ind].copy()\n",
    "    image_predicted = image_predicted.reshape((28, 56))\n",
    "    image_predicted = image_predicted * 255\n",
    "    image_predicted = image_predicted.astype('uint8')\n",
    "    dp.display_png(Image.fromarray(image_predicted))\n",
    "    p = model.predict(X_test[ind].reshape(1, 28, 56, 1))[0][0]\n",
    "    print(\"Prediction for %s: %s\" % (y_test[ind], p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Accuracy\n",
    "\n",
    "Evaluating the accuracy of the model can also be done by rounding the predicted values to the nearest integer and comparing these to the actual labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "for i in range(0, len(y_test)):\n",
    "    prediction = model.predict(X_test[i].reshape(1, X_test[i].shape[0], X_test[i].shape[1], 1))[0][0]\n",
    "    rounded_prediction = round(prediction)\n",
    "    floor_prediction = floor(prediction)\n",
    "    ceiling_prediction = ceil(prediction)\n",
    "    \n",
    "    abs_difference = abs(rounded_prediction-y_test[i])\n",
    "    \n",
    "    if abs_difference <= 1:\n",
    "        correct = correct + 1\n",
    "    else:\n",
    "        incorrect = incorrect + 1\n",
    "        \n",
    "print(\"Correct: %s, incorrect: %s\" % (correct, incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ten Fold Cross Validation\n",
    "\n",
    "In order to fully replicate the results in the paper, you can perform a 10 fold cross validation of the data by executing the code cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pairs = [str(x)+str(y) for x in range(10) for y in range(10)]\n",
    "train_counter = 0\n",
    "\n",
    "(X_train_keras, y_train_keras), (X_test_keras, y_test_keras) = mnist.load_data()\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=376483)\n",
    "kf.get_n_splits(unique_pairs)\n",
    "\n",
    "unique_pairs_np = np.asarray(unique_pairs)\n",
    "# Store network performance history and score for each of the 10 training runs.\n",
    "histories = []\n",
    "scores = []\n",
    "\n",
    "# Store accuracies measured in various ways\n",
    "accuracies_rounded = []\n",
    "accuracies_floor_ceil = []\n",
    "accuracies_leeway = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(unique_pairs):\n",
    "    test_set_pairs = unique_pairs_np[test_index]\n",
    "    train_set_pairs = unique_pairs_np[train_index]\n",
    "    \n",
    "    # Sanity checks\n",
    "    assert(len(test_set_pairs) == 10)\n",
    "    assert(len(train_set_pairs) == 90)\n",
    "    for test_set_pair in test_set_pairs:\n",
    "        assert(test_set_pair not in train_set_pairs)\n",
    "    \n",
    "    # If these pass we are good to go with data generation\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # Number of samples per permutation (e.g. there are 90 permutations in the train \n",
    "    # set so 1000*90 makes 90,000 training samples and 10*1000=10,000 test samples)\n",
    "    samples_per_permutation = 1000  \n",
    "\n",
    "    for train_set_pair in train_set_pairs:\n",
    "        for _ in range(samples_per_permutation):\n",
    "            rand_i = np.random.choice(np.where(y_train_keras == int(train_set_pair[0]))[0])\n",
    "            rand_j = np.random.choice(np.where(y_train_keras == int(train_set_pair[1]))[0])\n",
    "        \n",
    "            temp_image = np.zeros((28,56), dtype=\"uint8\")\n",
    "            temp_image[:,:28] = X_train_keras[rand_i]\n",
    "            temp_image[:,28:] = X_train_keras[rand_j]\n",
    "\n",
    "            X_train.append(temp_image)\n",
    "            y_train.append(y_train_keras[rand_i] + y_train_keras[rand_j])\n",
    "        \n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    samples_per_permutation = 1000\n",
    "    \n",
    "    for test_set_pair in test_set_pairs:\n",
    "        for _ in range(samples_per_permutation):\n",
    "            rand_i = np.random.choice(np.where(y_test_keras == int(test_set_pair[0]))[0])\n",
    "            rand_j = np.random.choice(np.where(y_test_keras == int(test_set_pair[1]))[0])\n",
    "        \n",
    "            temp_image = np.zeros((28,56), dtype=\"uint8\")\n",
    "            temp_image[:,:28] = X_test_keras[rand_i]\n",
    "            temp_image[:,28:] = X_test_keras[rand_j]\n",
    "            \n",
    "            X_test.append(temp_image)\n",
    "            y_test.append(y_test_keras[rand_i] + y_test_keras[rand_j])\n",
    "    \n",
    "    \n",
    "    # Explicitly convert to Numpy arrays, as they will be expected later\n",
    "    X_train = np.asarray(X_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "    # Reshape the data sets to a format suitable for Keras\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "    \n",
    "    # Some standard preprocessing things here.\n",
    "    # Reformat the images to use floating point values rather than integers between 0-255\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    \n",
    "    # Shuffling is always wise\n",
    "    X_train, y_train = utils.shuffle(X_train, y_train)\n",
    "    X_test, y_test = utils.shuffle(X_test, y_test)\n",
    "    \n",
    "    ######################################################\n",
    "    # NETWORK SETUP AND TRAINING\n",
    "    ######################################################\n",
    "    # Set up a few constants related to the size of the \n",
    "    # network, number of output classes and so on.\n",
    "    batch_size = 128\n",
    "    # This is a regression problem and we will \n",
    "    # use a single neuron as output: this network is not \n",
    "    # being trained as a classification problem.\n",
    "    num_classes = 1               \n",
    "    epochs = 100\n",
    "    img_rows, img_cols = np.shape(X_train)[1], np.shape(X_train)[2]\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "    ######################################################\n",
    "    # Set up the network itself\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))  # Default is (3, 3)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))  # Default is 128\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    # Do not use softmax here, just specify one nueron\n",
    "    model.add(Dense(num_classes)) # CHANGED TO LINEAR, REMOVE TO GO BACK TO DEFAULT\n",
    "\n",
    "    ######################################################\n",
    "    # Choose an optimiser and configure it.\n",
    "    # Here we have initialised a number of optimisers, but \n",
    "    # we will use Root Mean Squared Propagation (RMSprop)\n",
    "    rms = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    sgd = optimizers.SGD(lr=0.0001, decay=1e-5, momentum=0.9, nesterov=True)\n",
    "    ada = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "    ndm = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "\n",
    "    ######################################################\n",
    "    # Compile the network.\n",
    "    # Note: As this is a regression problem, only mean squared error \n",
    "    # or mean absolute error can be used for the loss.\n",
    "    model.compile(loss=losses.mean_squared_error, optimizer=ada)\n",
    "    \n",
    "    ## LET'S TRAIN\n",
    "    histories.append(model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=0))\n",
    "    \n",
    "    print(\"RUN %s\" % train_counter)\n",
    "    \n",
    "    scores.append(model.evaluate(X_test, y_test, verbose=0))\n",
    "    print(model.evaluate(X_test, y_test, verbose=1))\n",
    "    \n",
    "    rounded_correct = 0\n",
    "    rounded_incorrect = 0\n",
    "    floor_ceil_correct = 0\n",
    "    floor_ceil_incorrect = 0\n",
    "    leeway_correct = 0\n",
    "    leeway_incorrect = 0\n",
    "\n",
    "    for i in range(0, len(y_test)):\n",
    "        prediction = model.predict(X_test[i].reshape(1, X_test[i].shape[0], X_test[i].shape[1], 1))[0][0]\n",
    "\n",
    "        rounded_prediction = round(prediction)\n",
    "        floor_prediction = floor(prediction)\n",
    "        ceiling_prediction = ceil(prediction)\n",
    "\n",
    "        # Rounded to the nearest integer\n",
    "        if rounded_prediction == y_test[i]:\n",
    "            rounded_correct += 1\n",
    "        else:\n",
    "            rounded_incorrect += 1\n",
    "\n",
    "        # Floor or ceiling\n",
    "        if (floor_prediction == y_test[i]) or (ceiling_prediction == y_test[i]):\n",
    "            floor_ceil_correct += 1\n",
    "        else:\n",
    "            floor_ceil_incorrect += 1\n",
    "\n",
    "        # Leeway of 1\n",
    "        abs_difference = abs(rounded_prediction-y_test[i])\n",
    "\n",
    "        if abs_difference <= 1:\n",
    "            leeway_correct += 1\n",
    "        else:\n",
    "            leeway_incorrect += 1\n",
    "\n",
    "    accuracies_rounded.append((rounded_correct, rounded_incorrect))\n",
    "    accuracies_floor_ceil.append((floor_ceil_correct, floor_ceil_incorrect))\n",
    "    accuracies_leeway.append((leeway_correct, leeway_incorrect))\n",
    "\n",
    "    print(\"Correct (rounded): %s, Incorrect (rounded): %s\" % (rounded_correct, rounded_incorrect))        \n",
    "    print(\"Correct (floor/ceiling): %s, Incorrect (floor/ceiling): %s\" % (floor_ceil_correct, floor_ceil_incorrect))\n",
    "    print(\"Correct (leeway): %s, Incorrect (leeway): %s\" % (leeway_correct, leeway_incorrect))\n",
    "    \n",
    "    print(\"END %s\\n\" % train_counter)\n",
    "    \n",
    "    train_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_round = np.asarray(accuracies_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_floor = np.asarray(accuracies_floor_ceil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_leeway = np.asarray(accuracies_leeway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_leeway[:,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for history in histories:\n",
    "    print(\"Train loss: %s Test loss: %s\" % (history.history[\"loss\"][-1], history.history[\"val_loss\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_counter = 1\n",
    "for history in histories:\n",
    "    score = history.history['val_loss'][-1]\n",
    "    plt.xlim(0,len(history.history['loss'])-1)\n",
    "    plt.plot(history.history['loss'], linestyle='--', linewidth=3)\n",
    "    plt.plot(history.history['val_loss'], linewidth=3)\n",
    "    plt.title('Loss on Test/Training Set')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training Set', 'Test Set (Loss @ Final Epoch: '+ str(\"%.2f\"%score) +')'], loc='upper right')\n",
    "    plt.savefig(\"/tmp/loss-100-epochs-%s-fold.pdf\" % h_counter)\n",
    "    plt.close()\n",
    "    h_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for viewing this notebook. \n",
    "\n",
    "For questions please contact the maintainer of this repository, for contact details please see <https://github.com/mdbloice>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
