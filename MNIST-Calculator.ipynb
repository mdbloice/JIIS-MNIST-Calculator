{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Calculator\n",
    "\n",
    "This notebook can be used to reproduce all the experiments for the paper _Performing arithmetic using a neural network trained on combinations of hand written digits_. \n",
    "\n",
    "First we import the packages we require:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras imports for the convolutional neural network for later.\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "\n",
    "# Additional packages\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import utils  # Used for utils.shuffle() that can shuffle data + labels in unison.\n",
    "from sklearn.model_selection import KFold\n",
    "# from tabulate import tabulate\n",
    "import IPython.display as dp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (18, 6)\n",
    "%matplotlib inline\n",
    "from scipy.stats import itemfreq\n",
    "from math import floor, ceil\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Generate Training and Test Data\n",
    "\n",
    "Now that the appropriate libraries have been loaded, we will use scikit-learn's built-in MNIST dataset for generating the data in these experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train_keras, y_train_keras), (X_test_keras, y_test_keras) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this has loaded, we can take a look at an image from the dataset like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x10E88E9D0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(X_train_keras[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST images are $28 \\times 28$ pixels in size, and the combined images that will be used for training and testing will be two MNIST images placed side by side. The dimensions of a single MNIST image can be seen as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_keras[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the images used in this experiment will be $28 \\times 56$ in size. \n",
    "\n",
    "To demonstrate how the combined images look, we can make a random combination and display it inline. First we create an empty $28 \\times 56$ image matrix and then populate it with two images from the MNIST training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAAcCAAAAAAR0CPiAAABwUlEQVR4nO2SzUtUURiHjwVJjYTcCLOsTR8uopoSSiFIWuSiKBVvi8BwX4uYTeTCwgqFaGFIiyEwiskoIzAwMJrNyERB1KJoIaG3MphFk2YkU7+HaTE2du85/0DQu3rf33Mezse9xvyvv6thiKE9TrLc87ye/ofr7xQXzts0npf0xYo3bTmZvAdAMMK3TLO1YO9H9DWnphXheHeexfrV2d7eWB/VVu2fFnrRIbrDwJsEIDu2MOe8xW1JSF1PGY6Q1hun4GXMbE+6vIY8pBN82nWseDfKVlckOeHcrfQuj6oOn1trDPP2w14hvczpbUuRe91R6lHK4rE0h1xe5ahmW9bU/REz9orNc8HN0xVW3CQdKA9O0bTNwtnaaJolvTQUmXCdasc4XN8Qzo780JmlCQ26RFPdKZ6EI1+fy4eo7GO8yikaU6DQHBGnyt5FBS1Oa2fvY3gV/ii+Bha7eEoPnFr94AzwcyycHicoNYk8t1zausR7gOdHI7mvwrX4Rn80YGq40dZqDr4FyLZZP48vaeadpIleW/PuTwJkWlfarO6ZhJQbsNG+kQ8A3y/HXFcwtReErm51kH7gTd+laqf279dvmI/k/eW0GDoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=56x28 at 0x1AC7B1C70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty 28 * 56 element matrix\n",
    "test_image = np.zeros((28,56), dtype=\"uint8\")\n",
    "\n",
    "# Populate the empty matrix with data from two image matrices from the training set\n",
    "test_image[:,:28] = X_train_keras[0]\n",
    "test_image[:,28:] = X_train_keras[1]\n",
    "Image.fromarray(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, to generate the data we merely combine two MNIST images into one image.\n",
    "\n",
    "Now we generate our trainging pairs and test pairs indices. \n",
    "\n",
    "For the digits 0-9, there are 100 possible combinations of digits. More formally, we can say that the left side image can consist of the digits 0-9: $S_l = \\{0,1,2,3,4,5,6,7,8,9\\}$ and the right side consist of images from the set $S_r = \\{0,1,2,3,4,5,6,7,8,9\\}$. The total possible number of combinations is the Cartesian product of the two sets $D = S_l \\times S_r$. \n",
    "\n",
    "Here we first generate all possible pairs of combinations and then randomly select 90% of these for the training set and the remaining 10% are used for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the 100 unique pairs\n",
    "unique_pairs = [str(x)+str(y) for x in range(10) for y in range(10)]\n",
    "\n",
    "# Create 10 test set pairs\n",
    "test_set_pairs = []\n",
    "\n",
    "while(len(test_set_pairs) < 10):\n",
    "    pair_to_add = random.choice(unique_pairs)\n",
    "    if pair_to_add not in test_set_pairs:\n",
    "        test_set_pairs.append(pair_to_add)\n",
    "\n",
    "#Use the remaining 90 as training set pairs\n",
    "train_set_pairs = list(set(unique_pairs) - set(test_set_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the test set and training set pairs to ensure there is no overlap between the two data sets. These will be used later to generate the training and test data (they will be used as indices).\n",
    "\n",
    "The test looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['35', '49', '85', '36', '88', '12', '56', '29', '02', '31']\n"
     ]
    }
   ],
   "source": [
    "print(test_set_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the training set looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['98', '48', '76', '69', '81', '32', '55', '89', '50', '16', '92', '93', '62', '06', '79', '22', '00', '75', '24', '73', '45', '87', '08', '47', '27', '74', '13', '67', '11', '42', '54', '26', '61', '19', '05', '33', '40', '77', '51', '86', '43', '15', '78', '37', '17', '20', '30', '10', '63', '65', '53', '82', '71', '68', '57', '84', '25', '41', '44', '04', '01', '09', '59', '21', '99', '90', '03', '18', '39', '64', '96', '07', '80', '94', '83', '23', '66', '14', '28', '34', '97', '52', '91', '46', '60', '58', '38', '95', '72', '70']\n"
     ]
    }
   ],
   "source": [
    "print(train_set_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ten Fold Cross Validation\n",
    "\n",
    "Here we perform a 10-fold cross validation of the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN 0\n",
      "10000/10000 [==============================] - 2s 178us/step\n",
      "0.701783894992\n",
      "Correct (rounded): 8119, Incorrect (rounded): 1881\n",
      "Correct (floor/ceiling): 8551, Incorrect (floor/ceiling): 1449\n",
      "Correct (leeway): 9493, Incorrect (leeway): 507\n",
      "END 0\n",
      "\n",
      "RUN 1\n",
      "10000/10000 [==============================] - 2s 178us/step\n",
      "0.813004768658\n",
      "Correct (rounded): 4300, Incorrect (rounded): 5700\n",
      "Correct (floor/ceiling): 9089, Incorrect (floor/ceiling): 911\n",
      "Correct (leeway): 9623, Incorrect (leeway): 377\n",
      "END 1\n",
      "\n",
      "RUN 2\n",
      "10000/10000 [==============================] - 2s 179us/step\n",
      "0.757855157781\n",
      "Correct (rounded): 8095, Incorrect (rounded): 1905\n",
      "Correct (floor/ceiling): 8695, Incorrect (floor/ceiling): 1305\n",
      "Correct (leeway): 9512, Incorrect (leeway): 488\n",
      "END 2\n",
      "\n",
      "RUN 3\n",
      "10000/10000 [==============================] - 2s 180us/step\n",
      "1.09523425932\n",
      "Correct (rounded): 5508, Incorrect (rounded): 4492\n",
      "Correct (floor/ceiling): 7121, Incorrect (floor/ceiling): 2879\n",
      "Correct (leeway): 8656, Incorrect (leeway): 1344\n",
      "END 3\n",
      "\n",
      "RUN 4\n",
      "10000/10000 [==============================] - 2s 179us/step\n",
      "0.694715266299\n",
      "Correct (rounded): 8235, Incorrect (rounded): 1765\n",
      "Correct (floor/ceiling): 9356, Incorrect (floor/ceiling): 644\n",
      "Correct (leeway): 9528, Incorrect (leeway): 472\n",
      "END 4\n",
      "\n",
      "RUN 5\n",
      "10000/10000 [==============================] - 2s 180us/step\n",
      "0.431905881453\n",
      "Correct (rounded): 9223, Incorrect (rounded): 777\n",
      "Correct (floor/ceiling): 9549, Incorrect (floor/ceiling): 451\n",
      "Correct (leeway): 9676, Incorrect (leeway): 324\n",
      "END 5\n",
      "\n",
      "RUN 6\n",
      "10000/10000 [==============================] - 2s 180us/step\n",
      "0.831219066906\n",
      "Correct (rounded): 5432, Incorrect (rounded): 4568\n",
      "Correct (floor/ceiling): 8694, Incorrect (floor/ceiling): 1306\n",
      "Correct (leeway): 9538, Incorrect (leeway): 462\n",
      "END 6\n",
      "\n",
      "RUN 7\n",
      "10000/10000 [==============================] - 2s 180us/step\n",
      "0.579938846898\n",
      "Correct (rounded): 7486, Incorrect (rounded): 2514\n",
      "Correct (floor/ceiling): 9471, Incorrect (floor/ceiling): 529\n",
      "Correct (leeway): 9667, Incorrect (leeway): 333\n",
      "END 7\n",
      "\n",
      "RUN 8\n",
      "10000/10000 [==============================] - 2s 181us/step\n",
      "0.578246696866\n",
      "Correct (rounded): 8759, Incorrect (rounded): 1241\n",
      "Correct (floor/ceiling): 9433, Incorrect (floor/ceiling): 567\n",
      "Correct (leeway): 9591, Incorrect (leeway): 409\n",
      "END 8\n",
      "\n",
      "RUN 9\n",
      "10000/10000 [==============================] - 2s 181us/step\n",
      "0.753974978352\n",
      "Correct (rounded): 5748, Incorrect (rounded): 4252\n",
      "Correct (floor/ceiling): 8553, Incorrect (floor/ceiling): 1447\n",
      "Correct (leeway): 9623, Incorrect (leeway): 377\n",
      "END 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_pairs = [str(x)+str(y) for x in range(10) for y in range(10)]\n",
    "train_counter = 0\n",
    "\n",
    "(X_train_keras, y_train_keras), (X_test_keras, y_test_keras) = mnist.load_data()\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=376483)\n",
    "kf.get_n_splits(unique_pairs)\n",
    "\n",
    "unique_pairs_np = np.asarray(unique_pairs)\n",
    "# Store network performance history and score for each of the 10 training runs.\n",
    "histories = []\n",
    "scores = []\n",
    "\n",
    "# Store accuracies measured in various ways\n",
    "accuracies_rounded = []\n",
    "accuracies_floor_ceil = []\n",
    "accuracies_leeway = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(unique_pairs):\n",
    "    test_set_pairs = unique_pairs_np[test_index]\n",
    "    train_set_pairs = unique_pairs_np[train_index]\n",
    "    \n",
    "    # Sanity checks\n",
    "    assert(len(test_set_pairs) == 10)\n",
    "    assert(len(train_set_pairs) == 90)\n",
    "    for test_set_pair in test_set_pairs:\n",
    "        assert(test_set_pair not in train_set_pairs)\n",
    "    \n",
    "    # If these pass we are good to go with data generation\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # Number of samples per permutation (e.g. there are 90 permutations in the train \n",
    "    # set so 1000*90 makes 90,000 training samples and 10*1000=10,000 test samples)\n",
    "    samples_per_permutation = 1000  \n",
    "\n",
    "    for train_set_pair in train_set_pairs:\n",
    "        for _ in range(samples_per_permutation):\n",
    "            rand_i = np.random.choice(np.where(y_train_keras == int(train_set_pair[0]))[0])\n",
    "            rand_j = np.random.choice(np.where(y_train_keras == int(train_set_pair[1]))[0])\n",
    "        \n",
    "            temp_image = np.zeros((28,56), dtype=\"uint8\")\n",
    "            temp_image[:,:28] = X_train_keras[rand_i]\n",
    "            temp_image[:,28:] = X_train_keras[rand_j]\n",
    "\n",
    "            X_train.append(temp_image)\n",
    "            y_train.append(y_train_keras[rand_i] + y_train_keras[rand_j])\n",
    "        \n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    samples_per_permutation = 1000\n",
    "    \n",
    "    for test_set_pair in test_set_pairs:\n",
    "        for _ in range(samples_per_permutation):\n",
    "            rand_i = np.random.choice(np.where(y_test_keras == int(test_set_pair[0]))[0])\n",
    "            rand_j = np.random.choice(np.where(y_test_keras == int(test_set_pair[1]))[0])\n",
    "        \n",
    "            temp_image = np.zeros((28,56), dtype=\"uint8\")\n",
    "            temp_image[:,:28] = X_test_keras[rand_i]\n",
    "            temp_image[:,28:] = X_test_keras[rand_j]\n",
    "            \n",
    "            X_test.append(temp_image)\n",
    "            y_test.append(y_test_keras[rand_i] + y_test_keras[rand_j])\n",
    "    \n",
    "    \n",
    "    # Explicitly convert to Numpy arrays, as they will be expected later\n",
    "    X_train = np.asarray(X_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "    # Reshape the data sets to a format suitable for Keras\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "    \n",
    "    # Some standard preprocessing things here.\n",
    "    # Reformat the images to use floating point values rather than integers between 0-255\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    \n",
    "    # Shuffling is always wise\n",
    "    X_train, y_train = utils.shuffle(X_train, y_train)\n",
    "    X_test, y_test = utils.shuffle(X_test, y_test)\n",
    "    \n",
    "    ######################################################\n",
    "    # NETWORK SETUP AND TRAINING\n",
    "    ######################################################\n",
    "    # Set up a few constants related to the size of the \n",
    "    # network, number of output classes and so on.\n",
    "    batch_size = 128\n",
    "    # This is a regression problem and we will \n",
    "    # use a single neuron as output: this network is not \n",
    "    # being trained as a classification problem.\n",
    "    num_classes = 1               \n",
    "    epochs = 100\n",
    "    img_rows, img_cols = np.shape(X_train)[1], np.shape(X_train)[2]\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "    ######################################################\n",
    "    # Set up the network itself\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))  # Default is (3, 3)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))  # Default is 128\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    # Do not use softmax here, just specify one nueron\n",
    "    model.add(Dense(num_classes)) # CHANGED TO LINEAR, REMOVE TO GO BACK TO DEFAULT\n",
    "\n",
    "    ######################################################\n",
    "    # Choose an optimiser and configure it.\n",
    "    # Here we have initialised a number of optimisers, but \n",
    "    # we will use Root Mean Squared Propagation (RMSprop)\n",
    "    rms = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    sgd = optimizers.SGD(lr=0.0001, decay=1e-5, momentum=0.9, nesterov=True)\n",
    "    ada = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "    ndm = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "\n",
    "    ######################################################\n",
    "    # Compile the network.\n",
    "    # Note: As this is a regression problem, only mean squared error \n",
    "    # or mean absolute error can be used for the loss.\n",
    "    model.compile(loss=losses.mean_squared_error, optimizer=ada)\n",
    "    \n",
    "    ## LET'S TRAIN\n",
    "    histories.append(model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=0))\n",
    "    \n",
    "    print(\"RUN %s\" % train_counter)\n",
    "    \n",
    "    scores.append(model.evaluate(X_test, y_test, verbose=0))\n",
    "    print(model.evaluate(X_test, y_test, verbose=1))\n",
    "    \n",
    "    rounded_correct = 0\n",
    "    rounded_incorrect = 0\n",
    "    floor_ceil_correct = 0\n",
    "    floor_ceil_incorrect = 0\n",
    "    leeway_correct = 0\n",
    "    leeway_incorrect = 0\n",
    "\n",
    "    for i in range(0, len(y_test)):\n",
    "        prediction = model.predict(X_test[i].reshape(1, X_test[i].shape[0], X_test[i].shape[1], 1))[0][0]\n",
    "\n",
    "        rounded_prediction = round(prediction)\n",
    "        floor_prediction = floor(prediction)\n",
    "        ceiling_prediction = ceil(prediction)\n",
    "\n",
    "        # Rounded to the nearest integer\n",
    "        if rounded_prediction == y_test[i]:\n",
    "            rounded_correct += 1\n",
    "        else:\n",
    "            rounded_incorrect += 1\n",
    "\n",
    "        # Floor or ceiling\n",
    "        if (floor_prediction == y_test[i]) or (ceiling_prediction == y_test[i]):\n",
    "            floor_ceil_correct += 1\n",
    "        else:\n",
    "            floor_ceil_incorrect += 1\n",
    "\n",
    "        # Leeway of 1\n",
    "        abs_difference = abs(rounded_prediction-y_test[i])\n",
    "\n",
    "        if abs_difference <= 1:\n",
    "            leeway_correct += 1\n",
    "        else:\n",
    "            leeway_incorrect += 1\n",
    "\n",
    "    accuracies_rounded.append((rounded_correct, rounded_incorrect))\n",
    "    accuracies_floor_ceil.append((floor_ceil_correct, floor_ceil_incorrect))\n",
    "    accuracies_leeway.append((leeway_correct, leeway_incorrect))\n",
    "\n",
    "    print(\"Correct (rounded): %s, Incorrect (rounded): %s\" % (rounded_correct, rounded_incorrect))        \n",
    "    print(\"Correct (floor/ceiling): %s, Incorrect (floor/ceiling): %s\" % (floor_ceil_correct, floor_ceil_incorrect))\n",
    "    print(\"Correct (leeway): %s, Incorrect (leeway): %s\" % (leeway_correct, leeway_incorrect))\n",
    "    \n",
    "    print(\"END %s\\n\" % train_counter)\n",
    "    \n",
    "    train_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_round = np.asarray(accuracies_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_floor = np.asarray(accuracies_floor_ceil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_leeway = np.asarray(accuracies_leeway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9490.7000000000007"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_leeway[:,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11',\n",
       "       '12', '13', '14', '15', '16', '17', '19', '20', '21', '22', '23',\n",
       "       '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '35',\n",
       "       '37', '39', '40', '41', '42', '46', '47', '48', '49', '50', '51',\n",
       "       '52', '53', '55', '56', '57', '58', '59', '60', '61', '62', '63',\n",
       "       '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74',\n",
       "       '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85',\n",
       "       '86', '87', '88', '89', '90', '92', '93', '94', '95', '96', '97',\n",
       "       '98', '99'],\n",
       "      dtype='|S2')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0632535266088 Test loss: 1.10720102777\n",
      "Train loss: 0.0623196623021 Test loss: 0.69361052804\n",
      "Train loss: 0.0661115046157 Test loss: 0.773401923752\n",
      "Train loss: 0.0607607447558 Test loss: 0.784598394775\n",
      "Train loss: 0.0694891444554 Test loss: 0.956139478493\n",
      "Train loss: 0.0553421644602 Test loss: 0.773241580486\n",
      "Train loss: 0.0803066827628 Test loss: 1.21507503805\n",
      "Train loss: 0.0674270050183 Test loss: 0.727858560181\n",
      "Train loss: 0.0602696421915 Test loss: 0.946476239204\n",
      "Train loss: 0.0709666392962 Test loss: 0.555621409512\n"
     ]
    }
   ],
   "source": [
    "for history in histories:\n",
    "    print(\"Train loss: %s Test loss: %s\" % (history.history[\"loss\"][-1], history.history[\"val_loss\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_counter = 1\n",
    "for history in histories:\n",
    "    score = history.history['val_loss'][-1]\n",
    "    plt.xlim(0,len(history.history['loss'])-1)\n",
    "    plt.plot(history.history['loss'], linestyle='--', linewidth=3)\n",
    "    plt.plot(history.history['val_loss'], linewidth=3)\n",
    "    plt.title('Loss on Test/Training Set')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training Set', 'Test Set (Loss @ Final Epoch: '+ str(\"%.2f\"%score) +')'], loc='upper right')\n",
    "    plt.savefig(\"/tmp/loss-100-epochs-%s-fold.pdf\" % h_counter)\n",
    "    plt.close()\n",
    "    h_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pairs = np.asarray(unique_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['01', '18', '34', '36', '38', '43', '44', '45', '54', '91'],\n",
       "      dtype='|S2')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_pairs[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are 90 training set pairs and 10 test set pairs\n",
    "assert(len(test_set_pairs) == 10)\n",
    "assert(len(train_set_pairs) == 90)\n",
    "\n",
    "# Ensure no test set pairs appear in the training set pairs:\n",
    "for test_set in test_set_pairs:\n",
    "    assert(test_set not in train_set_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPERIMENT: Train and test with the same permutations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our permutation pairs, we can begin generating the data.\n",
    "\n",
    "We use the `train_set_pairs` and `test_set_pairs` arrays as indices to generate data, and in the example below we create 1000 samples for each test and train set permutation pair. So, there are for example exactly 1000 samples for every premutation, resulting in 100,000 images in total split 90/10 across the train set and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Number of samples per permutation (e.g. there are 90 permutations in the train set so 3000 * 90)\n",
    "samples_per_permutation = 1000  \n",
    "\n",
    "for train_set_pair in train_set_pairs:\n",
    "    for _ in range(samples_per_permutation):\n",
    "        rand_i = np.random.choice(np.where(y_train_keras == int(train_set_pair[0]))[0])\n",
    "        rand_j = np.random.choice(np.where(y_train_keras == int(train_set_pair[1]))[0])\n",
    "        \n",
    "        temp_image = np.zeros((28,56), dtype=\"uint8\")\n",
    "        temp_image[:,:28] = X_train_keras[rand_i]\n",
    "        temp_image[:,28:] = X_train_keras[rand_j]\n",
    "\n",
    "        X_train.append(temp_image)\n",
    "        y_train.append(y_train_keras[rand_i] + y_train_keras[rand_j])\n",
    "        \n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for test_set_pair in test_set_pairs:\n",
    "    for _ in range(samples_per_permutation):\n",
    "        rand_i = np.random.choice(np.where(y_test_keras == int(test_set_pair[0]))[0])\n",
    "        rand_j = np.random.choice(np.where(y_test_keras == int(test_set_pair[1]))[0])\n",
    "        \n",
    "        temp_image = np.zeros((28,56), dtype=\"uint8\")\n",
    "        temp_image[:,:28] = X_test_keras[rand_i]\n",
    "        temp_image[:,28:] = X_test_keras[rand_j]\n",
    "            \n",
    "        X_test.append(temp_image)\n",
    "        y_test.append(y_test_keras[rand_i] + y_test_keras[rand_j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we also generate our labels, which are the sum of the two images contained in each generated image. \n",
    "\n",
    "First, however, we will ensure we have generated the number of images is correct and that the label data matches the image data sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training set should be 90,000 images in size (90 permutations * 1000)\n",
    "# and the label data, y_train, must also be equal in length.\n",
    "assert(len(X_train) == samples_per_permutation * 90)\n",
    "assert(len(X_train) == len(y_train))\n",
    "\n",
    "# The test set should be 10,000 images in size (10 permutations * 1000)\n",
    "# and the label data, y_test, must also be equal in length\n",
    "assert(len(X_test) == samples_per_permutation * 10)\n",
    "assert(len(X_test) == len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also preview a generated image and its label so that we get an understanding of how the dataset looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image label: 14\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAAcCAAAAAAR0CPiAAABo0lEQVR4nO3TSyiEURgG4BfjUtO4\nlVssJDaDYiMLTLmFcomVsrCwoCxEU6Q02VgRo8klWViwmyZECeW2EEqmxsZ9EI1c0hiX38uClDnn\nt1I2vuX3/s853zn9B/iv36noZINh6I1c8w70A67VTBUUltQxdXlPuq2m+tDvUUCrm+S43OUekuTJ\nWHuaEGknedA8fPNSFKsRXcgxuVmp8ZOt2UirFjDTdpUvhro9cjlAOkvZY28ggC6S5ZJ4hreR8kNs\nvBkAQDdD1klGPeIoAESIkWvlY5Jq2oPFtIFn0Uhasjs7RUiTIQfIGGaxZJ42rgdankl64ryjRZJ0\ndF68skUCrVxe4IkptY+2z87X1RtLaz3KbFC+JdshOk0KsrhTco5n+HvDrS3Lo+KRbAYAVYnAdAUA\nwKnyCZZkZxwi5zUAkq+VIhUXfyuDTYojBoDPCCfUNtRTeqvFMQDQw7tCNWiUQwDQ9T85VR3MVPK8\ne/6+AHzT5/iQrupg5qHQ268BonrI3QJ1J4Xb9xN2F68Hw39wUqg/Jdmd8L0pPNo9kwAdwv/5J/UO\n3lOydlk0om8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=56x28 at 0x7F4DB84E7D10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = random.randint(0, len(X_test))\n",
    "print(\"Image label: \" + str(y_test[ind]))\n",
    "Image.fromarray(X_test[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each image sample consists of two MNIST images side by side, with each digit taken at random from the entire set of possible 3s in the MNIST data set. The image's label is the addition of the the two numbers.\n",
    "\n",
    "We can examine the frequency of the labels for the training and test set data also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "[[   2 1000]\n",
      " [   3 1000]\n",
      " [   6 1000]\n",
      " [   7 1000]\n",
      " [   9 1000]\n",
      " [  10 1000]\n",
      " [  11 1000]\n",
      " [  14 1000]\n",
      " [  16 1000]\n",
      " [  17 1000]]\n",
      "Training set:\n",
      "[[   0 1000]\n",
      " [   1 2000]\n",
      " [   2 2000]\n",
      " [   3 3000]\n",
      " [   4 5000]\n",
      " [   5 6000]\n",
      " [   6 6000]\n",
      " [   7 7000]\n",
      " [   8 9000]\n",
      " [   9 9000]\n",
      " [  10 8000]\n",
      " [  11 7000]\n",
      " [  12 7000]\n",
      " [  13 6000]\n",
      " [  14 4000]\n",
      " [  15 4000]\n",
      " [  16 2000]\n",
      " [  17 1000]\n",
      " [  18 1000]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set:\")\n",
    "print(itemfreq(y_test))\n",
    "print(\"Training set:\")\n",
    "print(itemfreq(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more images that sum to 7,8,9,10,11 and so on for obvious reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is important to note is that the network will be trained on 90 of the possible 100 permutations of the digits 0-9, and hence when tested will never have seen those permutations before. As we saw above, the `test_set_pairs` array contains 10 permuations that will never be seen during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will prepare the data for input into the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure we are using NumPy arrays\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "# Reshape the data sets to a format suitable for Keras\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "\n",
    "# Reformat the images to use floating point values rather than integers between 0-255\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling the data is always good practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = utils.shuffle(X_train, y_train)\n",
    "X_test, y_test = utils.shuffle(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network\n",
    "\n",
    "A very similar network to that used in the seminal LeNet5 experiment is used, except that we will be training this network as a regression problem rather than a classification problem. More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Set up a few constants related to the size of the \n",
    "# network, number of output classes and so on.\n",
    "batch_size = 128\n",
    "# This is a regression problem and we will \n",
    "# use a single neuron as output: this network is not \n",
    "# being trained as a classification problem.\n",
    "num_classes = 1               \n",
    "epochs = 100\n",
    "img_rows, img_cols = np.shape(X_train)[1], np.shape(X_train)[2]\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "\n",
    "######################################################\n",
    "# Set up the network itself\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))  # Default is (3, 3)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))  # Default is 128\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# Do not use softmax here, just specify one nueron\n",
    "model.add(Dense(num_classes, activation='linear')) # CHANGED TO LINEAR, REMOVE TO GO BACK TO DEFAULT\n",
    "\n",
    "\n",
    "######################################################\n",
    "# Choose an optimiser and configure it.\n",
    "# Here we have initialised a number of optimisers, but \n",
    "# we will use Root Mean Squared Propagation (RMSprop)\n",
    "rms = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "sgd = optimizers.SGD(lr=0.0001, decay=1e-5, momentum=0.9, nesterov=True)\n",
    "ada = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "ndm = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "\n",
    "\n",
    "######################################################\n",
    "# Compile the network.\n",
    "# Note: As this is a regression problem, only mean squared error \n",
    "# or mean absolute error can be used for the loss.\n",
    "model.compile(loss=losses.mean_squared_error, optimizer=ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Tests\n",
    "\n",
    "Some experiments. ** DO NOT RUN NOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = (img_rows * img_cols)\n",
    "\n",
    "X_train = X_train.reshape(len(X_train), 28*56)\n",
    "X_test = X_test.reshape(len(X_test), 28*56)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2000, input_shape=(28*56, ), activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(num_classes))\n",
    "\n",
    "rms = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "sgd = optimizers.SGD(lr=0.0001, decay=1e-5, momentum=0.9, nesterov=True)\n",
    "ada = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "ndm = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "\n",
    "model.compile(loss=losses.mean_absolute_error, optimizer=ndm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train the network. This will take some time, especially if you are not using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "90000/90000 [==============================] - 26s 285us/step - loss: 7.5706 - val_loss: 4.3498\n",
      "Epoch 2/100\n",
      "90000/90000 [==============================] - 17s 184us/step - loss: 3.9056 - val_loss: 4.9889\n",
      "Epoch 3/100\n",
      "90000/90000 [==============================] - 17s 187us/step - loss: 3.0883 - val_loss: 5.9190\n",
      "Epoch 4/100\n",
      "90000/90000 [==============================] - 17s 190us/step - loss: 2.5739 - val_loss: 19.7419\n",
      "Epoch 5/100\n",
      "90000/90000 [==============================] - 16s 183us/step - loss: 2.2252 - val_loss: 19.7223\n",
      "Epoch 6/100\n",
      "90000/90000 [==============================] - 16s 182us/step - loss: 1.9645 - val_loss: 18.3465\n",
      "Epoch 7/100\n",
      "90000/90000 [==============================] - 17s 184us/step - loss: 1.7629 - val_loss: 26.4703\n",
      "Epoch 8/100\n",
      "90000/90000 [==============================] - 17s 184us/step - loss: 1.5743 - val_loss: 21.9779\n",
      "Epoch 9/100\n",
      "90000/90000 [==============================] - 17s 184us/step - loss: 1.4832 - val_loss: 23.7118\n",
      "Epoch 10/100\n",
      "90000/90000 [==============================] - 16s 182us/step - loss: 1.3556 - val_loss: 21.3159\n",
      "Epoch 11/100\n",
      "90000/90000 [==============================] - 16s 182us/step - loss: 1.2631 - val_loss: 17.0811\n",
      "Epoch 12/100\n",
      "90000/90000 [==============================] - 17s 185us/step - loss: 1.1759 - val_loss: 20.8674\n",
      "Epoch 13/100\n",
      "90000/90000 [==============================] - 16s 181us/step - loss: 1.0961 - val_loss: 23.4607\n",
      "Epoch 14/100\n",
      "90000/90000 [==============================] - 17s 188us/step - loss: 1.0367 - val_loss: 19.5031\n",
      "Epoch 15/100\n",
      "90000/90000 [==============================] - 17s 185us/step - loss: 0.9693 - val_loss: 21.4708\n",
      "Epoch 16/100\n",
      "90000/90000 [==============================] - 16s 183us/step - loss: 0.9177 - val_loss: 17.4635\n",
      "Epoch 17/100\n",
      "90000/90000 [==============================] - 17s 188us/step - loss: 0.8683 - val_loss: 19.9994\n",
      "Epoch 18/100\n",
      "90000/90000 [==============================] - 16s 180us/step - loss: 0.8133 - val_loss: 18.2780\n",
      "Epoch 19/100\n",
      "90000/90000 [==============================] - 16s 183us/step - loss: 0.7793 - val_loss: 19.2915\n",
      "Epoch 20/100\n",
      "90000/90000 [==============================] - 17s 186us/step - loss: 0.7354 - val_loss: 17.4118\n",
      "Epoch 21/100\n",
      "90000/90000 [==============================] - 17s 189us/step - loss: 0.6916 - val_loss: 12.8162\n",
      "Epoch 22/100\n",
      "90000/90000 [==============================] - 17s 187us/step - loss: 0.6470 - val_loss: 15.7436\n",
      "Epoch 23/100\n",
      "90000/90000 [==============================] - 18s 196us/step - loss: 0.6115 - val_loss: 15.6602\n",
      "Epoch 24/100\n",
      "90000/90000 [==============================] - 17s 194us/step - loss: 0.5832 - val_loss: 15.6385\n",
      "Epoch 25/100\n",
      "90000/90000 [==============================] - 17s 193us/step - loss: 0.5434 - val_loss: 15.2239\n",
      "Epoch 26/100\n",
      "90000/90000 [==============================] - 17s 194us/step - loss: 0.5227 - val_loss: 11.7407\n",
      "Epoch 27/100\n",
      "90000/90000 [==============================] - 17s 185us/step - loss: 0.4868 - val_loss: 13.8838\n",
      "Epoch 28/100\n",
      "90000/90000 [==============================] - 16s 181us/step - loss: 0.4666 - val_loss: 11.1895\n",
      "Epoch 29/100\n",
      "90000/90000 [==============================] - 16s 181us/step - loss: 0.4461 - val_loss: 13.1355\n",
      "Epoch 30/100\n",
      "90000/90000 [==============================] - 17s 189us/step - loss: 0.4216 - val_loss: 10.4954\n",
      "Epoch 31/100\n",
      "90000/90000 [==============================] - 16s 182us/step - loss: 0.4089 - val_loss: 8.7755\n",
      "Epoch 32/100\n",
      "90000/90000 [==============================] - 16s 181us/step - loss: 0.3916 - val_loss: 10.1917\n",
      "Epoch 33/100\n",
      "90000/90000 [==============================] - 16s 179us/step - loss: 0.3759 - val_loss: 9.1417\n",
      "Epoch 34/100\n",
      "90000/90000 [==============================] - 16s 182us/step - loss: 0.3548 - val_loss: 8.5110\n",
      "Epoch 35/100\n",
      "90000/90000 [==============================] - 16s 178us/step - loss: 0.3432 - val_loss: 6.8800\n",
      "Epoch 36/100\n",
      "90000/90000 [==============================] - 16s 181us/step - loss: 0.3323 - val_loss: 7.3578\n",
      "Epoch 37/100\n",
      "90000/90000 [==============================] - 16s 178us/step - loss: 0.3232 - val_loss: 6.9110\n",
      "Epoch 38/100\n",
      "90000/90000 [==============================] - 16s 179us/step - loss: 0.3087 - val_loss: 9.0157\n",
      "Epoch 39/100\n",
      "90000/90000 [==============================] - 16s 179us/step - loss: 0.3007 - val_loss: 7.5536\n",
      "Epoch 40/100\n",
      "90000/90000 [==============================] - 16s 180us/step - loss: 0.2917 - val_loss: 6.7846\n",
      "Epoch 41/100\n",
      "90000/90000 [==============================] - 16s 177us/step - loss: 0.2790 - val_loss: 6.7223\n",
      "Epoch 42/100\n",
      "90000/90000 [==============================] - 16s 180us/step - loss: 0.2723 - val_loss: 7.1793\n",
      "Epoch 43/100\n",
      "90000/90000 [==============================] - 16s 182us/step - loss: 0.2643 - val_loss: 5.8878\n",
      "Epoch 44/100\n",
      "90000/90000 [==============================] - 16s 181us/step - loss: 0.2544 - val_loss: 5.5915\n",
      "Epoch 45/100\n",
      "90000/90000 [==============================] - 16s 178us/step - loss: 0.2466 - val_loss: 5.7019\n",
      "Epoch 46/100\n",
      "90000/90000 [==============================] - 16s 180us/step - loss: 0.2363 - val_loss: 5.4703\n",
      "Epoch 47/100\n",
      "90000/90000 [==============================] - 16s 179us/step - loss: 0.2320 - val_loss: 5.9711\n",
      "Epoch 48/100\n",
      "90000/90000 [==============================] - 16s 179us/step - loss: 0.2267 - val_loss: 4.7858\n",
      "Epoch 49/100\n",
      "90000/90000 [==============================] - 16s 178us/step - loss: 0.2128 - val_loss: 5.5839\n",
      "Epoch 50/100\n",
      "90000/90000 [==============================] - 16s 181us/step - loss: 0.2044 - val_loss: 5.6383\n",
      "Epoch 51/100\n",
      "90000/90000 [==============================] - 16s 181us/step - loss: 0.2022 - val_loss: 5.1604\n",
      "Epoch 52/100\n",
      "90000/90000 [==============================] - 16s 181us/step - loss: 0.1950 - val_loss: 4.9330\n",
      "Epoch 53/100\n",
      "90000/90000 [==============================] - 16s 182us/step - loss: 0.1867 - val_loss: 4.2721\n",
      "Epoch 54/100\n",
      "90000/90000 [==============================] - 16s 179us/step - loss: 0.1850 - val_loss: 3.3735\n",
      "Epoch 55/100\n",
      "90000/90000 [==============================] - 16s 182us/step - loss: 0.1785 - val_loss: 4.9486\n",
      "Epoch 56/100\n",
      "90000/90000 [==============================] - 16s 181us/step - loss: 0.1742 - val_loss: 3.6442\n",
      "Epoch 57/100\n",
      "90000/90000 [==============================] - 16s 181us/step - loss: 0.1663 - val_loss: 3.5038\n",
      "Epoch 58/100\n",
      "90000/90000 [==============================] - 16s 177us/step - loss: 0.1637 - val_loss: 3.7158\n",
      "Epoch 59/100\n",
      "90000/90000 [==============================] - 16s 177us/step - loss: 0.1588 - val_loss: 4.3521\n",
      "Epoch 60/100\n",
      "90000/90000 [==============================] - 16s 178us/step - loss: 0.1580 - val_loss: 3.4611\n",
      "Epoch 61/100\n",
      "90000/90000 [==============================] - 16s 177us/step - loss: 0.1548 - val_loss: 3.1578\n",
      "Epoch 62/100\n",
      "90000/90000 [==============================] - 17s 186us/step - loss: 0.1475 - val_loss: 3.2657\n",
      "Epoch 63/100\n",
      "90000/90000 [==============================] - 16s 182us/step - loss: 0.1440 - val_loss: 3.0348\n",
      "Epoch 64/100\n",
      "90000/90000 [==============================] - 16s 177us/step - loss: 0.1406 - val_loss: 2.7187\n",
      "Epoch 65/100\n",
      "90000/90000 [==============================] - 16s 177us/step - loss: 0.1348 - val_loss: 2.8792\n",
      "Epoch 66/100\n",
      "90000/90000 [==============================] - 16s 177us/step - loss: 0.1351 - val_loss: 3.0390\n",
      "Epoch 67/100\n",
      "90000/90000 [==============================] - 16s 181us/step - loss: 0.1328 - val_loss: 2.1487\n",
      "Epoch 68/100\n",
      "90000/90000 [==============================] - 16s 178us/step - loss: 0.1266 - val_loss: 2.8975\n",
      "Epoch 69/100\n",
      "90000/90000 [==============================] - 16s 180us/step - loss: 0.1229 - val_loss: 2.5574\n",
      "Epoch 70/100\n",
      "90000/90000 [==============================] - 16s 176us/step - loss: 0.1241 - val_loss: 2.3821\n",
      "Epoch 71/100\n",
      "90000/90000 [==============================] - 16s 179us/step - loss: 0.1192 - val_loss: 2.2686\n",
      "Epoch 72/100\n",
      "90000/90000 [==============================] - 16s 183us/step - loss: 0.1171 - val_loss: 2.3094\n",
      "Epoch 73/100\n",
      "90000/90000 [==============================] - 16s 182us/step - loss: 0.1150 - val_loss: 2.1924\n",
      "Epoch 74/100\n",
      "90000/90000 [==============================] - 16s 182us/step - loss: 0.1135 - val_loss: 2.2451\n",
      "Epoch 75/100\n",
      "90000/90000 [==============================] - 16s 178us/step - loss: 0.1075 - val_loss: 2.2240\n",
      "Epoch 76/100\n",
      "90000/90000 [==============================] - 16s 177us/step - loss: 0.1100 - val_loss: 2.1567\n",
      "Epoch 77/100\n",
      "90000/90000 [==============================] - 16s 177us/step - loss: 0.1062 - val_loss: 1.8285\n",
      "Epoch 78/100\n",
      "90000/90000 [==============================] - 16s 179us/step - loss: 0.1037 - val_loss: 1.8592\n",
      "Epoch 79/100\n",
      "90000/90000 [==============================] - 16s 176us/step - loss: 0.1035 - val_loss: 1.9021\n",
      "Epoch 80/100\n",
      "90000/90000 [==============================] - 16s 177us/step - loss: 0.1011 - val_loss: 1.6511\n",
      "Epoch 81/100\n",
      "90000/90000 [==============================] - 16s 176us/step - loss: 0.0974 - val_loss: 1.6648\n",
      "Epoch 82/100\n",
      "90000/90000 [==============================] - 17s 184us/step - loss: 0.0996 - val_loss: 1.7116\n",
      "Epoch 83/100\n",
      "90000/90000 [==============================] - 17s 190us/step - loss: 0.0978 - val_loss: 1.5071\n",
      "Epoch 84/100\n",
      "90000/90000 [==============================] - 17s 190us/step - loss: 0.0960 - val_loss: 1.5494\n",
      "Epoch 85/100\n",
      "90000/90000 [==============================] - 17s 188us/step - loss: 0.0966 - val_loss: 1.8000\n",
      "Epoch 86/100\n",
      "90000/90000 [==============================] - 17s 187us/step - loss: 0.0938 - val_loss: 1.5958\n",
      "Epoch 87/100\n",
      "90000/90000 [==============================] - 17s 187us/step - loss: 0.0902 - val_loss: 1.5337\n",
      "Epoch 88/100\n",
      "90000/90000 [==============================] - 16s 180us/step - loss: 0.0904 - val_loss: 1.2669\n",
      "Epoch 89/100\n",
      "90000/90000 [==============================] - 17s 187us/step - loss: 0.0904 - val_loss: 1.8323\n",
      "Epoch 90/100\n",
      "90000/90000 [==============================] - 17s 188us/step - loss: 0.0891 - val_loss: 1.5230\n",
      "Epoch 91/100\n",
      "90000/90000 [==============================] - 17s 184us/step - loss: 0.0869 - val_loss: 1.6433\n",
      "Epoch 92/100\n",
      "90000/90000 [==============================] - 16s 179us/step - loss: 0.0827 - val_loss: 1.5039\n",
      "Epoch 93/100\n",
      "90000/90000 [==============================] - 16s 177us/step - loss: 0.0860 - val_loss: 1.5090\n",
      "Epoch 94/100\n",
      "90000/90000 [==============================] - 16s 179us/step - loss: 0.0850 - val_loss: 1.4535\n",
      "Epoch 95/100\n",
      "90000/90000 [==============================] - 16s 178us/step - loss: 0.0822 - val_loss: 1.3191\n",
      "Epoch 96/100\n",
      "90000/90000 [==============================] - 16s 179us/step - loss: 0.0805 - val_loss: 1.6091\n",
      "Epoch 97/100\n",
      "90000/90000 [==============================] - 17s 189us/step - loss: 0.0813 - val_loss: 1.5653\n",
      "Epoch 98/100\n",
      "90000/90000 [==============================] - 17s 186us/step - loss: 0.0795 - val_loss: 1.3660\n",
      "Epoch 99/100\n",
      "90000/90000 [==============================] - 17s 185us/step - loss: 0.0790 - val_loss: 1.3562\n",
      "Epoch 100/100\n",
      "90000/90000 [==============================] - 16s 178us/step - loss: 0.0782 - val_loss: 1.2317\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the network's score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 182us/step\n",
      "\n",
      "0.55562\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"\\n%.5f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional: You may want to save the model and weights for later use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"/home/marcus/mnistcalc_model_best.h5\")\n",
    "model.save_weights(\"/home/marcus/mnistcalc_weights_best.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the loss on the training set and test to get an overview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff40c254fd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histories[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUO8j1JKLOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff42dab8e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = histories[9] % for 10 fold.\n",
    "plt.xlim(0,len(history.history['loss'])-1)\n",
    "plt.plot(history.history['loss'], linestyle='--', linewidth=3)\n",
    "plt.plot(history.history['val_loss'], linewidth=3)\n",
    "plt.title('Loss on Test/Training Set')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Set', 'Test Set (Loss @ Final Epoch: '+ str(\"%.2f\"%score) +')'], loc='upper right')\n",
    "plt.savefig(\"/tmp/loss-100-epochs-10fold-title.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can select random images from the test set, and see how well the network predicts their summation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAAcCAAAAAAR0CPiAAABi0lEQVR4nO2PTSiDcRzHv3vabNOe\nZWJNCheKw7OTmpoVByV2QUyRleQmSZGSi5PbDq5KKWk7LIokpLmJWhwcpmS1yMuBzab9HnPY6/Py\nX7k5+F2e7/P9/D7/5/8A//ObMWx8ORjI/DapVrcKAAAHkSoGUJOw5SNXbNtDZz3q+5r8lhB5VIr2\nwyreBYDrwsumVJyezYWhpPLU5gDRrgnABJFTxmJ8LlyvKjzDOdGeGQD8RLVSth7Ip5RHIQaJjo0A\n4E2Tn5OysKsguuVedZxeO7NbRFtSZrkwFsSOQqvNPgYMOAgBwGCT4jKjl0lAawAwk4rIxRvSuiMA\nNFYjEJaKjR7+CuNC4v3Dmn5WHDsf+RRFUcyI4sOIDOmm1k78npZ6Hr4ST1NITguAYAZe2S+WjG+4\njkHGRFrhGAyAb7+YpWv9SO98s8XuJ4bY1wv/LdtDZZQBjihqKePhzq7eC0naLueZYvriS8lVuWUd\n7pcEtthgU+9NRCTSIltsy+hVe26B6HSuoswX4+riX58fkC53uZW6YokAAAAASUVORK5CYII=\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for index 3671: 7.46396 (Actual: 8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAAcCAAAAAAR0CPiAAABfUlEQVR4nGNgGAUUAkaZptv//v37\nN10MXcLLEsZUf76SEU2WI3LWXyhYyoQqxf43H8b0/vtXFU3j1L8IIAcTZCHSsY87GRi0M5EEiNH4\n9iHDuiMbGBi8MTX64NVYVwdnfv4DY0E8q49Q5YdTP080A8PsZ6hiV/62CzEwMDAwiDEcxwwcCFBd\n+vfvG1k4FxL0//8xXHzAwMDAoH7TTpBhz1cGhuNLUcyusGEwE2Zg2OP/A9W0f38xwE1keavnUNFp\nHKgaW1E1Pd2/39MFWX4hXKoW1alM1gy8kCQgZMSwr+nlLTT/Oe6BsV5rv8UaAgyuf/82YYpyNE0x\nNDQ0vPb3718H7PoYXP/+1cYhxcBw9u/fv1OgbCYM2b13cWqcxsDAcBmH3Pq/a3Bpk065+PfvXwko\nDy2tSmpi08K7RpeBgYFNkIGB4fo37Kba/8VmowIiptRhYph+7MPlVAYGhhsKt3Fr/I6p/sNlBgYG\nhvstCvqP/+EwUqExFr3gGDQAACFatG7XKdZwAAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for index 8536: 7.09811 (Actual: 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAAcCAAAAAAR0CPiAAABRUlEQVR4nGNgGIlA99wqEXL0eXz5\n968Gm4Txw0dGSFwmNOnelezZW3WxaZSQlbFA4rKgyqamsycs5TTH4RpDnDaazmROXopD14XfDHdw\nSDHwnviXyMDAcmglNknvf/+8cGm0+fdCmIHB/18ndo0/eHBp3PTPl4GBYdE1Zuwav+HSZ/jjMwMD\ng8/vHqyyaBqRA6eArYuBgSH24ypcJuMAYf9PMjAwxP2LxC49898fA6wSzFseyDAw2P04wYldY8N/\nHBpj/81jYJB/+80IqywDg/e/76xYJTY8UWNguP8vHIc+huB/P4Sxidv9WsmgdPnfFBZskhAXYY+O\n3n8xts/+XRbApQ9PdOTuk5ir+wGnRhxA8M2/fx97sSYZKJj47yQ24aoft5qd8Rr97F8UMhcWFvoT\nKgi4iYPhJwEVOMC8x3LkaRxwAABuq2d/DKZHVwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for index 8823: 1.5383 (Actual: 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAAcCAAAAAAR0CPiAAABS0lEQVR4nO2SIU8DQRCFv0tQYDm5\nYKltDRc86FbcP+B+QCWVrS0JCo/ckCAbXFXNaQKuydZwBIfd2UFwadLr3mERzIhN3tu37+7NwH+1\nVhID7flJAzk6uybVz6fnzsdydbtAWrx48eLFj34RzneBmYS6PwZdQqvZLiB+2/dRhbGawVhXDVwl\nVGVZfgUJUaFxqhlGNd/FR+KXfeChzdGqWrBqG3hvcQykMwkSpjGh0xXk6kyMLErx4h8Po4bOwKoZ\nKQCD6ifVtFW3nwwAVZ1qsU+NVXPMWGOGp1X7HDNVZ62q7kUDDCJzPKhPA8YAm4vNvjBJkre7197w\nKrLaZuVsnrvmCLeOfQDx8h7jmceTIV2HKZAGCVWMbxshDMXfFpN1y+Zk6qIfCqBSd2RzTMsPAjCt\nU41tju3QwURFpbyJMKZTly7ELy87Lvzl+gaxk+BcAfku0QAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for index 6644: 6.38974 (Actual: 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAAcCAAAAAAR0CPiAAABwElEQVR4nOWT3SuDURzHv5tZKTeb\n5iUsWkJNqEkKiZC3SOLGH8CK5AI3koQbN3JBmsIlN/KWlHapCG2LG8JE2MzMGLPHby40z57nPBe4\n5Hd1zud7Pr9zTp0D/M2auLus+YUWY+aI+kIzxbc99Wo+NlyHP99vj2hI9mMNaKX30YiwuaBHrEYV\npyzEy5CH8bI3NWM94UABQKc26ICoejgzlDIAuB9hxHKNz8TAdfosl91qJiLydzFLIrdpX0gUALRw\nerDoOjiOmy0CEGhcY8SGfGyxYr3qwgEAGbuyIDxyrsVyKRZLYDUNqPLOrHO3TFNAuWLpbyxLMZws\nloojM7kd9Pbg4x+AVKnOT7VCUsgR0WYBMnlRLiG6090GIUmUw9tWvY378y8kJcK/1yQEtUDHNAe0\npDCiYTmXX6eNuRZ4+jK45gGglWehR175fBBCUcVTCZkCsS7+EQBgzPHtiE+3tRIaFZnobVgY9r7f\nelMhq/SSkblWt3+1GMjSD9j85GgXhQnGDbJ3ThLNMB7yXilgsxEROcerxGG0Lu0kGKSnSSUrotlM\nROS8sqRJhEDy4NKCPhzw3yqpQn1jP/Jwkt7/rA9VP6qsJqzLhQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for index 2864: 9.88649 (Actual: 9)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAAcCAAAAAAR0CPiAAABuElEQVR4nO2RP0hbURTGjzaWmBBt\nabo4RaWIQy2FLCVD6BQFTXGwgoNCKYWggehQLXTJ0KX/eKGT2bqVLA5PpFBoBpdMFnQQQoxCB1ur\nLQmhQfOdE4fUl3dzH3EpdOkHD873u/fHfe9dov/5m9nIOMCe96d3ghMDLTQwdrNZYmU4iDNydm9P\nEirs2EHEKteKcBCXa7LwUCSseh1A2mpxOIh3f8ur7oysuhR6C7Cd6CSOHEnc496VkIqjivhdF28f\nyVs3rUihS8HeLGD6L1qMNdG9LWtXKXgsUyofAzDYGFOGsaWJ3g3JXSFfUT50qmKemR89yNaZ+c+z\noKw/FpknWhdZCoX6bHy2CqBcBgPgw89gFHoV8fqmVA4OaiIikr/R5Ek0wgBOIq/BSBARkfXnf92P\nvvF7ib59PKQvlab4vH4x/XixfTxJRLukJSa1Z74WFv7JzMyc8je+t14c1sWaLOpw1DRNc3yQiCjw\nFeCX+pan/MmrU1uSANY8Gu6HBNt6xAAMjfrystKpb7YlVWfOOmApuXRqj6HdIRHRUKXa394jg0tP\nrGK93HT3u/1LRKJEWmdzuUu1f5xzfOHiOO52t6IAAAAASUVORK5CYII=\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for index 862: 7.86016 (Actual: 8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAAcCAAAAAAR0CPiAAABmUlEQVR4nOWTPUibURSGD9LGGFEc\nlC6CsRQ0KA6mgx26dKgOgn+LUAcHCXURImKxY3Goi2sydakKlUwuIkKJgqVUJ0myRlDR2HbyB5Lv\nnicOVQpfjoV2tGe5cF8e3vfce47If1l9hd1/oCKxdXVDf0vFCyjKK1utn1va8zzP87z3fkmdOnXZ\nkMkNfee2Dv1aLrv7dl7pNrCGLYXLjdnWiRPKM341FBKJOrXARz/hQ4tUT5XgTcBK1Gc7SvyKDmn+\nAsxWG3LTYsGlrB7DzwYePxzNgxcL+qRIPBmN5pSs5Sff4HilCGsVcYbPf72p6SfSlgcovvPbiSQV\nRUmZmIisAywbwtipunRCXcTEQsslgANLi8Sei2ypOacdGSBxRrHrrkBJl2+svB05AhaCKei5E7R+\ncdLBSX+wfudPoDE3rxWWAvIkAzthG6udRw/8UdsdrDaMJ65gs+739YObc7hRpL23rVze/uED01Ui\nnfvNIpef4ucVXk+1rCh6UbnDDsDxdbrWCtlyqk4zWWv3Bz+XDj++eFljd3ev6xp7L+Lz4wV+BgAA\nAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for index 1898: 11.4764 (Actual: 11)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAAcCAAAAAAR0CPiAAABjUlEQVR4nO2TSyjEURTGv2FGeU5m\nQUkW3pKlFHlEyliImZSyIAsLCxaKsJGULMlmFM1CpCyY2KDY2JgU5VHqL4zMaPIcMpk5h4XU8D+3\nLC2c5fe7v3O/bl3gf34xFZvMfkeh+oD9nt8vW3VxVzAQCBEFZo2y1u8Pnw8tsCfxJxiZB2qnTuit\nUtKiuulpJgMGF3fKe0vpOVPKq5mLAGCRe2Sxnu6k2PLALcZPseCrw/cThZiWRFMSlsIADNFuTbww\nWbtJk/JU5jYAaOcOuamTXGJusHppMhY5V26ThM1tocMUeSMSXLxscfOgCFeIDvIUImLtZz7254vM\nfhGkJ4fJoFAnmIdVa9HnJSoWSYzj9eVtRymiLkQn0gMYF0PruaPhcrW5QVSvT7M8XivQyM1qsZce\nc/U9d/fzAVgiROEvHJ7qotX0cg2ALbJ7JO80O+OtGNPvMq9pAJByfyTXHCDPHlGGHmz5SuJgago4\nZQ9GH9GLLUoPsj18vO3m2yqFiNbrhhoRlM1pzOPJKu9vzwfxXpW2vsEEWwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for index 9103: 11.3635 (Actual: 11)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAAcCAAAAAAR0CPiAAAB4UlEQVR4nN2TT0hUURTGv4mZIlfW\nPKHIESuyMEbIok2FvoVEiwFbJiGYQ0jmRlDENm3atJwYCVdSLkQkXBgJbXRTJpEE5SxcFEkOGIij\nhk/H79PFTDNv3n1i2zqbc+/5nR/3z7sP+P8j8DdN0WYgHnnxbvCgxpieFidXeke2SZLcGao9QPyi\nycK4K8NizPv333l4CABwfcslOiRTyWQ0fGmUzLb4aEeep/M7fKUS8WtNOQDg8KT/knXS3AkAqFiW\nkoXy3a0/4/BHcsL0gmOabwIAjEvLF4qgNZ7LVh/JNlNMSgkAwI2s1G3g+OcFkj+OGeDMqlbOAwBe\nS4p5ccM6SXKzv8oDAgPSAyBYFujKSp3Gs7j2Kf9BOjwgIjlnawffpKy3ki6aJwnbtm0PkU889Rlp\n58OalIpJSleaIgCg+hed5tLSN0mS3jtTUqZuHw+4Rc6UFC5vSNLPRP2zhLS4r4fT5Npxd2FY0nZL\nZRBXF6V73vaqkEtkjxvdlobLAGBAmg15PHuj/1SoIP6OutnJ9GgFANSsaPexd8G2DDnSbVmWZdWT\nmaMl8FwutUtLEeNk7XRHfqu5HwkLudQIzJl3M9u3tFuYrE8bHMBLOQ1+ddx/9J0kOWa+YgDATbX6\ng3869gDfNACpIru8IQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for index 7790: 6.76048 (Actual: 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAAcCAAAAAAR0CPiAAABbElEQVR4nO3SPyiEcRzH8Q9djm7U\nlcufjsJgEZMMlMKIlEhSNguF8ifuImWRchmvLP4e3SWllD9hsRwDC0nXFTcIeULy/WC4G07P97FY\nDL7T0+/V+/v86nmA//nluEr2yc+LdhXta59DFl1dVESEcp+jaTd5o3eVb7zbGGvyXMmmxifknB46\nvc1OANiVHT18ywUAbAdd+oJvoS0JPqIAsFOVklGvda7ClEPt3E92AWh7JMPqC/1y7NTOW8hIWsG8\nkA9lZk0tDfA4U12ICBNzbiJH9ZIIe9x6OCnxLlBsomD8O8b69HI8SpLsNMtZqLuhddkgF/SynCQX\nU3UEPC8i/apkkbzIt+qAxmeJuTWYIC9/6IAVYVXiMfleNYPA6fVPocWMkpf635buAADYtnig8Sw5\noK8cWS0BgCmRbI0NGhV6eCdPYY83/P46o7LBdb3D8O2VCOV1WucjdliE9pw8n2+vt8iCa0N2C/kT\n8wX5xq1VdQuPdgAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for index 84: 7.11598 (Actual: 7)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    ind = random.randint(0, len(X_test)-1)\n",
    "    image_predicted = X_test[ind].copy()\n",
    "    image_predicted = image_predicted.reshape((28, 56))\n",
    "    image_predicted = image_predicted * 255\n",
    "    image_predicted = image_predicted.astype('uint8')\n",
    "    dp.display_png(Image.fromarray(image_predicted))\n",
    "    p = model.predict(X_test[ind].reshape(1, 28, 56, 1))[0][0]\n",
    "    print(\"Prediction for index %s: %s (Actual: %s)\" % (ind, p, y_test[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Accuracy\n",
    "\n",
    "Evaluating the accuracy of the model can also be done by rounding the predicted values to the nearest integer and comparing these to the actual labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 8517, incorrect: 1483\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "for i in range(0, len(y_test)):\n",
    "    prediction = model.predict(X_test[i].reshape(1, X_test[i].shape[0], X_test[i].shape[1], 1))[0][0]\n",
    "    rounded_prediction = round(prediction)\n",
    "    floor_prediction = floor(prediction)\n",
    "    ceiling_prediction = ceil(prediction)\n",
    "    \n",
    "    abs_difference = abs(rounded_prediction-y_test[i])\n",
    "    \n",
    "    if abs_difference <= 1:\n",
    "        correct = correct + 1\n",
    "    else:\n",
    "        incorrect = incorrect + 1\n",
    "        \n",
    "print(\"Correct: %s, incorrect: %s\" % (correct, incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8517"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 6235, incorrect: 3765\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "for i in range(0, len(y_test)):\n",
    "    prediction = model.predict(X_test[i].reshape(1, X_test[i].shape[0], X_test[i].shape[1], 1))[0][0]\n",
    "    rounded_prediction = round(prediction)\n",
    "    \n",
    "    if rounded_prediction == y_test[i]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "\n",
    "print(\"Correct: %s, incorrect: %s\" % (correct, incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Layers for their Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we examine a single filter (index 0) of the layer conv2d_1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To really perform a good sanity test, it is best to test the network on the **training set** so that we know we are learning a model that can predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_training_set_score = model.evaluate(X_train, y_train)\n",
    "print(\"\\n%.5f\" % _training_set_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how well the network performs per correct sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "\n",
    "for _ in range(len(X_test)):\n",
    "    p = model.predict(X_test[_].reshape(1, X_test[_].shape[0], X_test[_].shape[1], 1))[0][0]\n",
    "    #if floor(p) == y_test[_] or ceil(p) == y_test[_]:\n",
    "    #    correct += 1\n",
    "    if round(p) == y_test[_]:\n",
    "        correct += 1\n",
    "\n",
    "print(str(correct) + \" of \" + str(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct / float(len(X_test)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how well the network performs for each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = itemfreq(y_test)\n",
    "test_set_labels = []\n",
    "for f in freq:\n",
    "    test_set_labels.append(f[0])\n",
    "\n",
    "correct_by_label = dict()\n",
    "\n",
    "for l in test_set_labels:\n",
    "    correct_by_label[l] = 0\n",
    "\n",
    "for test_set_label in test_set_labels:\n",
    "    for _ in range(len(X_test)):\n",
    "        if y_test[_] == test_set_label:\n",
    "            p = model.predict(X_test[_].reshape(1, X_test[_].shape[0], X_test[_].shape[1], 1))[0][0]\n",
    "            if round(p) == y_test[_]:\n",
    "                correct_by_label[test_set_label] += 1\n",
    "\n",
    "correct_by_label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
